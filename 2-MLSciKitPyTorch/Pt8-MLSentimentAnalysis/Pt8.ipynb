{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72516e2",
   "metadata": {},
   "source": [
    "## IMDb Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6800db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE TIME SETUP SCRIPT\n",
    "# Obtain the dataset, unzip from dataset folder in local\n",
    "\n",
    "# import tarfile\n",
    "# with tarfile.open('dataset/aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "#     tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f128ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset into Pandas DataFrame\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "basepath = 'dataset/aclImdb'\n",
    "\n",
    "# labels = {'pos': 1, 'neg': 0} # Binary classification\n",
    "# pbar = pyprind.ProgBar(50000, stream=sys.stdout) # Total number of reviews\n",
    "# df = pd.DataFrame()\n",
    "# for s in ('train', 'test'): # Iterate through train and test sets\n",
    "#     for l in ('pos', 'neg'): # Iterate through train and test sets, positive and negative labels\n",
    "#         path = os.path.join(basepath, s, l) # Path to the reviews\n",
    "#         for file in sorted(os.listdir(path)): # Iterate through files in the path\n",
    "#             with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "#                 txt = infile.read() # Read the review text\n",
    "#             df = df.append([[txt, labels[l]]], ignore_index=True) # Append to DataFrame\n",
    "#             pbar.update()\n",
    "# df.columns = ['review', 'sentiment']\n",
    "\n",
    "# Label mapping: positive review = 1, negative review = 0\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "\n",
    "# Progress bar for 50,000 total reviews\n",
    "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
    "\n",
    "data = []  # List to hold tuples of (review text, sentiment label)\n",
    "\n",
    "# Loop through both 'test' and 'train' datasets\n",
    "for s in ('test', 'train'):\n",
    "    # Loop through 'pos' and 'neg' subfolders\n",
    "    for l in ('pos', 'neg'):\n",
    "        # Path to the specific sentiment folder\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        \n",
    "        # Loop through all review files in sorted order\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()  # Read review text\n",
    "            \n",
    "            # Append the review text and label to DataFrame\n",
    "            # df = df.append([[txt, labels[l]]], ignore_index=True) # Depreciated append\n",
    "            # Instead of appending, we collect data in a list for efficiency\n",
    "            data.append((txt, labels[l]))\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update()\n",
    "\n",
    "# Create DataFrame once at the end\n",
    "df = pd.DataFrame(data, columns=['review', 'sentiment'])\n",
    "\n",
    "# Name the columns: review text and sentiment label\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096b6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Shuffle the DataFrame rows for randomness to split the dataset into train and test sets\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8') # VFor conveenience, save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34bc5aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Make sure formatting is correct\n",
    "\n",
    "\n",
    "# Load the movie review dataset from CSV\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "\n",
    "# Rename columns in case they were saved as \"0\" and \"1\" instead of proper names\n",
    "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "\n",
    "# Display the first 3 rows\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c3db33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that dataframe contains all 50000 reviews:\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77557dce",
   "metadata": {},
   "source": [
    "### Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13fb331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To construct a bag-of-words model, can use count vectorizer from sklearn, which takes array of text data and constructs a matrix of token counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create a CountVectorizer instance\n",
    "count = CountVectorizer()\n",
    "# example text data\n",
    "text = ['The sun is shining', 'The weather is sweet', 'The sun is shining and the weather is sweet']\n",
    "# Fit and transform the text data to create a bag-of-words model\n",
    "bag_of_words = count.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21575573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da006ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words.toarray()) # This is also known as 1-gram model, where each word is a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a112b",
   "metadata": {},
   "source": [
    "####  term frequency-inverse document frequency (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b50c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.56 0.56 0.   0.43 0.  ]\n",
      " [0.   0.43 0.   0.   0.56 0.43 0.56]\n",
      " [0.4  0.48 0.31 0.31 0.31 0.48 0.31]]\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn also provides a tf-idf transformer that can be used in conjunction with CountVectorizer to create a tf-idf representation of the text data\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the TF-IDF transformer\n",
    "# use_idf=True → use inverse document frequency\n",
    "# norm='l2' → normalize each vector to unit length\n",
    "# smooth_idf=True → add 1 to document frequencies to avoid division by zero\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "\n",
    "# Set NumPy print options to show only 2 decimal places\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Transform 'docs' into term-frequency matrix using 'count' (CountVectorizer),\n",
    "# then convert it to TF-IDF representation and print as an array\n",
    "print(tfidf.fit_transform(count.fit_transform(text)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6329821",
   "metadata": {},
   "source": [
    "#### Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0ecbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should first clean the text by stripping it of unwanted characters\n",
    "\n",
    "# As you can see, the text contains HTML markup and punctuation that should be removed for better analysis.\n",
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e28571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use python's regular expression library to remove HTML markup and punctuation\n",
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    # Extract emoticons like :) ;-) :-D etc.\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    # Remove non-word characters, lowercase everything\n",
    "    # Then append emoticons at the end without hyphens\n",
    "    text = (\n",
    "        re.sub(r'[\\W]+', ' ', text.lower()) +\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    )\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1942729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the prprocessor works\n",
    "preprocessor(df.loc[0, 'review'][-50:])\n",
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef910a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessor to all movie reviews in dataframe\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bbcc1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits text simply into words\n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "309a6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another useful technique is word stemming, which is the process of transforming words to their root form\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b57ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another useful trick is stop word removal, which is the process of removing common words that do not contribute to the meaning of the text, such as \"the\", \"is\", \"in\", etc.\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a8df467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can apply english stop word set as follows:\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')  # List of common English stop words\n",
    "\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')\n",
    " if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e66687",
   "metadata": {},
   "source": [
    "#### Training logistic regression model for document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e276193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First divide dataframe of cleaned text into 25000 training and 25,000 test reviews\n",
    "# First 25,000 reviews for training\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "\n",
    "# Remaining reviews for testing\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6a363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001308336F4C0>; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001308336F4C0>; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001308336F4C0>; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001308336F4C0>; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001308336F4C0>; total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001308336F600>; total time=  48.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001308336F600>; total time=  47.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001308336F600>; total time=  48.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001308336F600>; total time=  47.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001308336F600>; total time=  47.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001308336F4C0>; total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001308336F4C0>; total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001308336F4C0>; total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001308336F4C0>; total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x000001308336F4C0>; total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001308336F600>; total time=  47.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001308336F600>; total time=  48.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001308336F600>; total time=  49.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x000001308336F600>; total time=  50.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Next use GridSearchCV to find the best hyperparameters for the logistic regression model using 5-fold stratified cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TF-IDF vectorizer\n",
    "# - strip_accents=None: keeps accents\n",
    "# - lowercase=False: does not force lowercase (we already handle in preprocessor if needed)\n",
    "# - preprocessor=None: we use our custom preprocessor/tokenizer instead\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "# Define a smaller parameter grid to search over\n",
    "# We test different configurations for the TfidfVectorizer + Logistic Regression\n",
    "small_param_grid = [\n",
    "    {\n",
    "        # Unigrams only\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [None],  # No stop word removal\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],  # try raw split and stemming\n",
    "        'clf__penalty': ['l2'],  # Ridge penalty\n",
    "        'clf__C': [1.0, 10.0]    # Regularization strengths\n",
    "    },\n",
    "    {\n",
    "        # Same unigram setup, but testing stop word removal and TF vs raw counts\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [stop, None],  # Try removing stopwords or not\n",
    "        'vect__tokenizer': [tokenizer],    # only simple split\n",
    "        'vect__use_idf': [False],          # disable IDF weighting\n",
    "        'vect__norm': [None],              # disable length normalization\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Build a pipeline: TF-IDF vectorizer → Logistic Regression classifier\n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', LogisticRegression(solver='liblinear'))  # liblinear works well for bigger datasets compared to lbfgs\n",
    "])\n",
    "\n",
    "# Grid search with 5-fold cross-validation\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf,\n",
    "                           small_param_grid,\n",
    "                           scoring='accuracy',  # evaluate by accuracy\n",
    "                           cv=5,                # 5-fold CV\n",
    "                           verbose=2,           # show progress\n",
    "                           n_jobs=1)            # run in serial (could set to -1 for parallel)\n",
    "\n",
    "# Train grid search on training set\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e02dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x00000149D3D31A80>}\n",
      "Best cross-validation accuracy: 0.897\n",
      "Test accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "# Print best parameter set\n",
    "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
    "# Also print best VC and Test accuracy on test dataset\n",
    "print(f'Best cross-validation accuracy: {gs_lr_tfidf.best_score_:.3f}')\n",
    "print(f'Test accuracy: {gs_lr_tfidf.score(X_test, y_test):.3f}')  # Evaluate on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8351bc",
   "metadata": {},
   "source": [
    "#### Working on bigger data - online algos and out-of-core learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0991b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous section's construction of feature vectors for 50000 review dataset takes a while, so we can use out-of-core learning, which allows us to fit the classifier incrementally on chunks of data\n",
    "# similar to stochastic gradient descent, where we can fit the model on small batches of data instead of the entire dataset at once, we will be using partial_fit method of the classifier to stream documents from local drive and train model incrementally\n",
    "# First, define tokenizer function\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load English stop words\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    Custom tokenizer that:\n",
    "    1. Removes HTML tags\n",
    "    2. Extracts emoticons\n",
    "    3. Converts to lowercase and removes non-word characters\n",
    "    4. Appends emoticons back to the text\n",
    "    5. Splits text into tokens\n",
    "    6. Removes stopwords\n",
    "    \"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "\n",
    "    # Find emoticons like :-) or :D\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "\n",
    "    # Lowercase and remove non-word characters\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower())\n",
    "\n",
    "    # Append emoticons to the end (without '-')\n",
    "    text = text + ' '.join(emoticons).replace('-', '')\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547cafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next define a generator function that reads in and returns one document at a time\n",
    "def stream_docs(path):\n",
    "    \"\"\"\n",
    "    Lazily stream documents from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV file (expects 'review,sentiment' format).\n",
    "\n",
    "    Yields:\n",
    "        tuple: (text, label) where text is the review and label is the sentiment (int).\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        # Skip header row\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            # Extract text (all but last 3 chars: \",0\" or \",1\") and label (last char)\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319602f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify streamdocs function works\n",
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72633db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define function get_minibatch taht takes a document stream from stream_docs and returns particular number of documments specified by size parameter\n",
    "def get_minibatch(doc_stream, size):\n",
    "    \"\"\"\n",
    "    Retrieve a minibatch of documents from a stream.\n",
    "\n",
    "    Args:\n",
    "        doc_stream (iterator): Stream of (text, label) tuples.\n",
    "        size (int): Number of documents to fetch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (docs, y)\n",
    "            - docs: list of review texts\n",
    "            - y: list of labels\n",
    "        If stream ends, returns (None, None).\n",
    "    \"\"\"\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)  # Get next document\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None  # End of stream\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2620ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannot use CountVectorizer for out-of-core learning as it requires holding the complete vocab in memory. Can instead use HashingVectorizer, which uses a hash function to map tokens to feature indices, allowing it to handle large vocabularies without storing them in memory\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# HashingVectorizer for transforming text into feature vectors\n",
    "vect = HashingVectorizer(\n",
    "    decode_error='ignore',   # Ignore decoding errors\n",
    "    n_features=2**21,        # Large number of features for hashing\n",
    "    preprocessor=None,       # Use default preprocessing\n",
    "    tokenizer=tokenizer      # Custom tokenizer defined earlier\n",
    ")\n",
    "\n",
    "# SGDClassifier with logistic regression loss\n",
    "clf = SGDClassifier(loss='log_loss', random_state=1) # use log_loss instead of log for newer versions of scikit-learn\n",
    "\n",
    "# Create a document stream generator from the CSV dataset\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b330fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:16\n"
     ]
    }
   ],
   "source": [
    "# Now can start the out-of-core learning using following code:\n",
    "import pyprind\n",
    "import numpy as np\n",
    "\n",
    "# Progress bar for 45 iterations (45 minibatches)\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "# Binary classification labels\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "# Train in minibatches of 1000 documents\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    \n",
    "    # Break loop if no more data\n",
    "    if not X_train:\n",
    "        break\n",
    "    \n",
    "    # Transform text into feature vectors\n",
    "    X_train = vect.transform(X_train)\n",
    "    \n",
    "    # Incrementally train classifier\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    \n",
    "    # Update progress bar\n",
    "    pbar.update() # using PyPrind to show progrees of learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c60d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "# Get a test minibatch of 5000 documents\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "\n",
    "# Transform test data using the same HashingVectorizer\n",
    "X_test = vect.transform(X_test)\n",
    "\n",
    "# Evaluate accuracy of the classifier\n",
    "print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81bd123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated accuracy: 0.894\n"
     ]
    }
   ],
   "source": [
    "clf = clf.partial_fit(X_test, y_test) # use the last 5000 documents to update the model further\n",
    "print(f'Updated accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0e417",
   "metadata": {},
   "source": [
    "## LatentDirichletAllocation (LDA) with SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3d59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In following code, will restrict analysis to 10 topics, which is a good number for most datasets, but can be adjusted\n",
    "# First, load dataset into a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Load the movie review dataset from CSV\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "\n",
    "# On some systems, pandas may default to numeric column names; rename them for clarity\n",
    "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "\n",
    "# Optional: check the first few rows to verify\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881cb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use count vectorizer to create a bag-of-words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english',  # Remove common English stop words\n",
    "                        max_df=.1, # Ignore terms that appear in more than 10% of the documents # rationale is that these might be common words that do not contribute to the meaning of the text\n",
    "                        max_features=5000)     # Limit to 5000 most frequent words # Limit the dimensionality of dataset to improve inference performed by LDA # This and max_df are arbritrarily chosen hyperparameters\n",
    "X = count.fit_transform(df['review'].values)  # Transform reviews into feature vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use a LDA estimator to fit the model to the data\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,  # Number of topics\n",
    "                                 learning_method='batch',  # Batch learning method (can also use 'online' for large datasets)\n",
    "                                 random_state=123) \n",
    "# FYI: Sciki-Learn's LDA implementation uses expectation-maximization (EM) algorithm to estimate the parameters of the model, which is a common approach for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8952e34",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LatentDirichletAllocation' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# After fitting LDA, now have access to components_ attribute, which contains the topic-word distributions, meaning it stores matrix containing word importance for each of 10 topics in increasing order\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mlda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomponents_\u001b[49m.shape\n",
      "\u001b[31mAttributeError\u001b[39m: 'LatentDirichletAllocation' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "# After fitting LDA, now have access to components_ attribute, which contains the topic-word distributions, meaning it stores matrix containing word importance for each of 10 topics in increasing order\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to analyze results, print the 5 most important words for each topic\n",
    "# Number of top words to display per topic\n",
    "n_top_words = 5\n",
    "\n",
    "# Get feature (word) names from the CountVectorizer\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "# Loop through each topic in the LDA model\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {(topic_idx + 1)}:')\n",
    "    # argsort() returns indices that would sort the topic's word importance values\n",
    "    # [:-n_top_words - 1:-1] slices the top n words in descending order, as importance values are sorted in ascending order\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] \n",
    "    print(' '.join(top_words))  # Join top words into a string and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm that categories make sense, plot three movies from horror movie category# Get indices of movies with the highest probability for topic 5 (assumed \"horror\")\n",
    "horror = X_topics[:, 5].argsort()[::-1]  # Descending order\n",
    "\n",
    "# Print the top 3 \"horror\" movie reviews\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print(f'\\nHorror movie #{(iter_idx + 1)}:')\n",
    "    # Print first 300 characters of the review as a preview\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
